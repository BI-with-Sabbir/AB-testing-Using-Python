# ğŸ§ª A/B Testing Process with Optimum Sample Size Calculation

## ğŸ§­ Context / Table of Contents
- [ğŸ“Œ Project Overview](#-project-overview)
- [ğŸ¯ Project Objectives](#-project-objectives)
- [ğŸ” Problem Statement](#-problem-statement)
- [ğŸ—‚ï¸ Dataset Description](#-dataset-description)
- [ğŸ§ª A/B Testing Process](#-ab-testing-process)
- [ğŸ“Š Project Output](#-project-output)
- [ğŸ“ˆ Data Storytelling](#-data-storytelling)
- [ğŸ§  Decision-Making](#-decision-making)
- [ğŸŒ Project Impact](#-project-impact)
- [ğŸ’¼ Business Recommendations](#-business-recommendations)

---

## ğŸ“Œ Project Overview

![Project Banner](https://github.com/user-attachments/assets/24de73e6-517e-4293-bb34-bae0703a351f)

This project demonstrates a complete A/B testing workflow to evaluate the impact of a new feature on user behavior (Click-Through Rate - CTR). It covers optimum sample size calculation, baseline metric setup, randomized group assignment, simulation, hypothesis testing, and final recommendations.

---

## ğŸ¯ Project Objectives

- Design a statistically sound A/B testing framework.
- Calculate optimal sample size using confidence level, power, and MDE.
- Simulate test/control group data and CTR performance.
- Apply Two-Proportion Z-Test for result validation.
- Communicate outcomes and suggest business actions.

---

## ğŸ” Problem Statement

A digital product team wants to evaluate whether a proposed change (UI/feature/offer) improves the conversion rate. Instead of deploying it blindly, they want to validate the effect through a rigorous A/B testing process.

---

## ğŸ—‚ï¸ Dataset Description

| Dataset Name        | Purpose |
|---------------------|---------|
| `Activity_pretest.csv` | Logs user activity over 31 days to calculate DAU |
| `Ctr_pretest.csv`      | Provides baseline CTR mean and standard deviation |
| `Activity_all.csv`     | Tracks activity levels across test groups |
| `Assignments.csv`      | Records test/control group assignment for 8807 users |
| `Ctr_all.csv`          | Shows CTR values for both groups after rollout |

---

## ğŸ§ª A/B Testing Process

### ğŸ“ Step 1: Pre-Test Metrics
- Calculated **Daily Active Users (DAU)** from activity logs.
- Found baseline **CTR** = **33.0%**, SD = **1.73%**

### ğŸ“ Step 2: Define Parameters
- Baseline CTR: **10%**
- Minimum Detectable Effect (MDE): **3%**
- Significance Level (Î±): **0.05**
- Power (1-Î²): **80%**

### ğŸ“ Step 3: Sample Size Calculation
Used `statsmodels` to find that each group needs **at least 1,356 users**.

### ğŸ“ Step 4: Simulate & Assign Data
- Simulated outcomes for:
  - **Control Group**: 10% CTR
  - **Treatment Group**: 13% CTR
- Randomly assigned users into groups.

### ğŸ“ Step 5: Hypothesis Testing
- Applied **Two-Proportion Z-Test**
- **p-value = 0.0021** â†’ Statistically significant

---

## ğŸ“Š Project Output

Visualizations included:
- ğŸ“ˆ Daily Active Users trend  
- ğŸ¯ CTR distribution before & after test  
- ğŸ“Š Sample size calculator output  
- âœ… Significant uplift in Treatment group CTR

![DAU](https://github.com/user-attachments/assets/08f5c67e-e86c-4834-96e3-92fd5f1c6d5a)
![CTR](https://github.com/user-attachments/assets/636970ce-a235-43f2-9adf-3ec544acc622)
![Sample Size](https://github.com/user-attachments/assets/79da2051-ec54-4306-8dad-093c74b2d890)

---

## ğŸ“ˆ Data Storytelling

> â€œThe A/B test showed a **3% uplift in CTR**, backed by a **p-value of 0.0021**. With 1,350+ users per group and proper power/MDE setup, this confirms the effectiveness of the change. The data supports a full rollout.â€

---

## ğŸ§  Decision-Making

âœ”ï¸ **Decision**: Roll out to 100% of users

**Why?**
- Statistically significant uplift
- Minimum required power and sample size achieved
- Business value confirmed

---

## ğŸŒ Project Impact

- âœ… Strengthened culture of data-driven product testing
- âœ… Reduced guesswork in feature releases
- âœ… Set a reusable testing framework for future launches

---

## ğŸ’¼ Business Recommendations

1. **Standardize A/B Testing for All Feature Launches**
2. **Always Calculate Sample Size Before Running Tests**
3. **Track Guardrail Metrics (Bounce, Time-on-Page, etc.)**
4. **Document Learnings from Every Experiment**
5. **Use Controlled Rollouts (10â€“20% â†’ 100%)**

---

## ğŸ·ï¸ Tags

`A/B Testing` `Z-Test` `CTR Analysis` `Hypothesis Testing` `Product Analytics` `Experimentation` `Sample Size` `MDE` `Python` `statsmodels`

---
